{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Nisarg Thakkar\n",
    "\n",
    "Scrape the Dell laptop community, and find out how many posts are there for Inspiron, XPS and Latitude\n",
    "\n",
    "Scrape all discussion from the first 5 pages for the Inspiron sub-community. Find discussion title, author of the question,\n",
    "time of the question, author of the latest post, time of the latest post, number of kudos, number of replies, \n",
    "number of views, whether the question is solved or not (indicated using green checkmark). \n",
    "Then save these information to a csv file.\n",
    "\n",
    "https://www.dell.com/community/Laptops/ct-p/Laptops\n",
    "'''\n",
    "\n",
    "#import libraries\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os, csv\n",
    "import pandas as pd\n",
    "\n",
    "#Specify url, get the webpage and return it to varibale page\n",
    "page = urlopen(\"https://www.dell.com/community/Laptops/ct-p/Laptops\")\n",
    "#print(page)\n",
    "\n",
    "#parse the html page and store in variable soup\n",
    "soup = BeautifulSoup(page, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Inspiron': '12,271 posts', 'XPS': '5,505 posts', 'Latitude': '3,725 posts'}\n"
     ]
    }
   ],
   "source": [
    "# Scrape the Dell laptop community, and find out how many posts are there for Inspiron, XPS and Latitude\n",
    "\n",
    "d_name_post = {}\n",
    "j = [0, 1, 3]\n",
    "\n",
    "for i in j:    \n",
    "\n",
    "    names = soup.find_all('div', attrs = {\"cat-card-title\"})\n",
    "    names = names[i].text\n",
    "\n",
    "    i_post = soup.find_all('span', attrs = {'class': \"cat-card-posts\"})\n",
    "    i_post = i_post[i].text.strip()\n",
    "    \n",
    "    d_name_post[names] = i_post\n",
    "\n",
    "print(d_name_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Scrape all discussion from the first 5 pages for the Inspiron sub-community. Find discussion title, \n",
    " author of the question, time of the question, author of the latest post, time of the latest post, number of kudos, number of replies, \n",
    "number of views, whether the question is solved or not (indicated using green checkmark). \n",
    "Then save these information to a csv file.\n",
    "'''\n",
    "\n",
    "dic = {}\n",
    "#Specify url, get the webpage and return it to varibale page\n",
    "for pg_number in range(1, 6): \n",
    "    \n",
    "   \n",
    "    inspiron_page = urlopen(\"https://www.dell.com/community/Inspiron/bd-p/Inspiron/page/{}\".format(pg_number))\n",
    "    #print(page)\n",
    "\n",
    "    #parse the html page and store in variable soup\n",
    "    soup = BeautifulSoup(inspiron_page, 'html.parser')\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    whole_post = soup.find_all('div', attrs = {\"class\" : 'lia-component-messages-column-thread-info'})\n",
    "    #Getting a whole post row\n",
    "    \n",
    "    solved_container = soup.find_all('td', attrs = {'class' : 'triangletop lia-data-cell-secondary lia-data-cell-icon'})\n",
    "    #getting a container tag to find out if post is solved or not\n",
    "    \n",
    "   \n",
    "    #going through each post in that page\n",
    "    for z in range(len(whole_post)):\n",
    "        st = ((whole_post[z].text.strip()).replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"  \",\"\"))\n",
    "        #print(st)\n",
    "        date = soup.find_all('span', attrs = {'class' : 'DateTime lia-component-common-widget-date'})\n",
    "        date = date[z].text.strip()\n",
    "\n",
    "        views = soup.find_all('div', attrs = {'class' : 'lia-component-messages-column-topic-views-count'})\n",
    "        views = views[z].text.strip()[0:5] #getting views on that post\n",
    "\n",
    "        by_pos = st.find(\"by\")\n",
    "        title = st[:by_pos]\n",
    "        on_pos = st.find(\" on \")\n",
    "        author = st[by_pos + 3 : on_pos]\n",
    "        if len(author) >= 15:\n",
    "            author = 'None'\n",
    "\n",
    "        first_on_pos = st.find(\" on \")\n",
    "        last_on_pos = st.rfind(\" on \")\n",
    "        #print(first_on_pos, last_on_pos)\n",
    "        latest_comment_date = 'None' #Intialing it to 'None'\n",
    "        latest_auth = 'None'\n",
    "\n",
    "        if first_on_pos != last_on_pos:#finding if there're multiple dates to find out if someone commented on it or not        \n",
    "\n",
    "            last_by_pos = st.rfind(\" by \")\n",
    "            replies_pos = st.rfind(\"Reply\")\n",
    "\n",
    "            latest_comment_date = st[last_on_pos + 4 : last_by_pos]\n",
    "            #print('Latest comment date :', latest_comment_date)\n",
    "\n",
    "            if last_by_pos != by_pos: \n",
    "                latest_auth = st[last_by_pos + 4 : replies_pos - 1]\n",
    "                #print('Latest comment made by : ', latest_auth)\n",
    "\n",
    "\n",
    "        kudos = soup.find_all('td', attrs = {'class' : 'cKudosCountColumn lia-data-cell-secondary lia-data-cell-integer'})\n",
    "        #print(kudos)\n",
    "        kudos = kudos[z].text.strip()\n",
    "\n",
    "        message_count = soup.find_all('td', attrs = {'class' : 'cRepliesCountColumn lia-data-cell-secondary lia-data-cell-integer'})\n",
    "        message_count = message_count[2].text.strip()\n",
    "\n",
    "        row = soup.find_all('tr', {'class' :re.compile('lia-js-data-messageUid-\\d+')})\n",
    "        #print(len(row))\n",
    "        solved = row[z].find('td', attrs = {'aria-label' : 'This thread is solved'})\n",
    "   \n",
    "        if solved == None: #if solved tag exists, post is solved, if None return it does not\n",
    "            solved =\"Not Solved\"\n",
    "        else:\n",
    "            solved = 'solved'\n",
    "            \n",
    "        dic[title] = [author, date, latest_auth, latest_comment_date, kudos, message_count, views, solved]\n",
    "#print(dic)\n",
    "\n",
    "names = ['author', 'date', 'latest_author', 'latest_comment_date', 'kudos', 'message_count', 'views', 'solved'] \n",
    "\n",
    "\n",
    "(pd.DataFrame.from_dict(data=dic, orient='index')\n",
    ".to_csv('Nisarg_web_scrapping.csv', header= names ))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
